{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from keras.layers import Conv2D, Flatten, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tetris environment\n",
    "env = gym.make(\"ALE/Tetris-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state and action space sizes\n",
    "action_space = env.action_space.n # 5\n",
    "state_space = env.observation_space.shape # (210, 160, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 10000\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.999\n",
    "update_target_network_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model() -> tf.keras.Model:\n",
    "    \"\"\"Create a convolutional neural network model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.keras.Model\n",
    "        A sequential model with the following layers:\n",
    "        - Conv2D with 32 filters, kernel size of 8, stride of 4, and relu activation\n",
    "        - Conv2D with 64 filters, kernel size of 4, stride of 2, and relu activation\n",
    "        - Conv2D with 64 filters, kernel size of 3, stride of 1, and relu activation\n",
    "        - Flatten layer\n",
    "        - Dense layer with 512 units and relu activation\n",
    "        - Dense layer with 5 units and linear activation\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=state_space),\n",
    "        Conv2D(64, (4, 4), strides=(2, 2), activation='relu'),\n",
    "        Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(action_space, activation='linear')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "target_model = create_model()\n",
    "\n",
    "# Compile the model with an optimizer and loss function\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "# Initially, set the target model weights equal to the model's weights\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: tf.keras.Model, target_model: tf.keras.Model, minibatch: np.ndarray, discount_rate: float):\n",
    "    \"\"\"Train the model using the minibatch of transitions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : tf.keras.Model\n",
    "        The main neural network model that is being trained.\n",
    "    target_model : tf.keras.Model\n",
    "        The target neural network model that is used to predict the Q-values for the next state.\n",
    "    minibatch : np.ndarray\n",
    "        The minibatch of transitions to train the model on.\n",
    "    discount_rate : float\n",
    "        The discount rate to use when calculating the Q-values.\n",
    "    \"\"\"\n",
    "    # Extract information from the minibatch\n",
    "    states = np.array([transition[0] for transition in minibatch]) # (64, 210, 160, 3)\n",
    "    actions = np.array([transition[1] for transition in minibatch]) # (64,)\n",
    "    rewards = np.array([transition[2] for transition in minibatch]) # (64,)\n",
    "    next_states = np.array([transition[3] for transition in minibatch]) # (64, 210, 160, 3)\n",
    "    dones = np.array([transition[4] for transition in minibatch]) # (64,)\n",
    "\n",
    "    # Predict Q-values for starting state and next state\n",
    "    current_q_values = model.predict(states, verbose=0)\n",
    "    next_q_values = target_model.predict(next_states, verbose=0)\n",
    "\n",
    "    # Update Q-values for actions taken\n",
    "    for i in range(len(minibatch)):\n",
    "        if dones[i]:\n",
    "            next_q_values[i][actions[i]] = rewards[i]\n",
    "        else:\n",
    "            next_q_values[i][actions[i]] = rewards[i] + discount_rate * np.amax(next_q_values[i])\n",
    "\n",
    "\n",
    "    # Perform a gradient descent step\n",
    "    model.fit(states, next_q_values, epochs=1, verbose=0, batch_size=len(minibatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a step counter and replay memory\n",
    "step_counter = 0\n",
    "replay_memory = deque(maxlen=10000)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_threshold = random.uniform(0, 1)\n",
    "        # If exploration_threshold > exploration_rate, then exploitation\n",
    "        if exploration_threshold > exploration_rate:\n",
    "            q_values = model.predict(np.expand_dims(state, axis=0), verbose=0) # add batch dimension\n",
    "            action = np.argmax(q_values[0])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # Increment the step counter\n",
    "        step_counter += 1\n",
    "        \n",
    "        # Take action and observe the next state and reward\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Add the experience to replay memory\n",
    "        replay_memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Sample a minibatch from the replay buffer\n",
    "        if len(replay_memory) > batch_size:\n",
    "            minibatch = random.sample(replay_memory, batch_size)\n",
    "            # Train the model on the minibatch\n",
    "            train(model, target_model, minibatch, discount_rate)\n",
    "\n",
    "        if step_counter % update_target_network_steps == 0:\n",
    "            # Update the the target network with new weights\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "        # Decay the exploration rate\n",
    "        exploration_rate =  max(min_exploration_rate, exploration_rate * exploration_decay_rate)\n",
    "    # End of episode\n",
    "    print(f'Episode: {episode}, Exploration Rate: {exploration_rate:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
