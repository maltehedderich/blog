{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from glob import glob\n",
    "from time import perf_counter\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from keras.layers import Conv2D, Flatten, Dense\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tetris environment\n",
    "env = gym.make(\"ALE/Tetris-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model File Path\n",
    "model_path = Path(\"models\")\n",
    "\n",
    "# Define the state and action space sizes\n",
    "action_space = env.action_space.n # 5\n",
    "state_space = env.observation_space.shape # (210, 160, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_shape = (*state_space[:2], 1) # (210, 160, 1) as we will convert to grayscale\n",
    "num_episodes = 10000\n",
    "batch_size = 256\n",
    "learning_rate = 0.0001\n",
    "discount_rate = 0.99\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.99999\n",
    "update_target_network_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model() -> tf.keras.Model:\n",
    "    \"\"\"Create a convolutional neural network model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.keras.Model\n",
    "        A sequential model with the following layers:\n",
    "        - Conv2D with 32 filters, kernel size of 8, stride of 4, and relu activation\n",
    "        - Conv2D with 64 filters, kernel size of 4, stride of 2, and relu activation\n",
    "        - Conv2D with 64 filters, kernel size of 3, stride of 1, and relu activation\n",
    "        - Flatten layer\n",
    "        - Dense layer with 512 units and relu activation\n",
    "        - Dense layer with 5 units and linear activation\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=input_shape),\n",
    "        Conv2D(64, (4, 4), strides=(2, 2), activation='relu'),\n",
    "        Conv2D(64, (3, 3), strides=(1, 1), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(action_space, activation='linear')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "target_model = create_model()\n",
    "\n",
    "# Compile the model with an optimizer and loss function\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "# Initially, set the target model weights equal to the model's weights\n",
    "target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: tf.keras.Model, target_model: tf.keras.Model, minibatch: np.ndarray, discount_rate: float):\n",
    "    \"\"\"Train the model using the minibatch of transitions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : tf.keras.Model\n",
    "        The main neural network model that is being trained.\n",
    "    target_model : tf.keras.Model\n",
    "        The target neural network model that is used to predict the Q-values for the next state.\n",
    "    minibatch : np.ndarray\n",
    "        The minibatch of transitions to train the model on.\n",
    "    discount_rate : float\n",
    "        The discount rate to use when calculating the Q-values.\n",
    "    \"\"\"\n",
    "    # Extract information from the minibatch\n",
    "    states = np.array([transition[0] for transition in minibatch]) # (64, 210, 160, 3)\n",
    "    actions = np.array([transition[1] for transition in minibatch]) # (64,)\n",
    "    rewards = np.array([transition[2] for transition in minibatch]) # (64,)\n",
    "    next_states = np.array([transition[3] for transition in minibatch]) # (64, 210, 160, 3)\n",
    "    dones = np.array([transition[4] for transition in minibatch]) # (64,)\n",
    "\n",
    "    # Predict Q-values for starting state and next state\n",
    "    current_q_values = model.predict(states, verbose=0)\n",
    "    next_q_values = target_model.predict(next_states, verbose=0)\n",
    "\n",
    "    target_q_values = current_q_values.copy()\n",
    "    for i in range(len(minibatch)):\n",
    "        if dones[i]:\n",
    "            target_q_values[i][actions[i]] = rewards[i]\n",
    "        else:\n",
    "            target_q_values[i][actions[i]] = rewards[i] + discount_rate * np.amax(next_q_values[i])\n",
    "    \n",
    "    model.fit(states, target_q_values, epochs=1, verbose=0, batch_size=len(minibatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: tf.keras.Model, name_prefix: str, episode: int):\n",
    "    \"\"\"Save the model weights to the specified path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : tf.keras.Model\n",
    "        The model to save the weights of.\n",
    "    name_prefix : str\n",
    "        Prefix to use when saving the model weights.\n",
    "    episode : int\n",
    "        The episode number to use when saving the model weights.\n",
    "    \"\"\"\n",
    "    model.save_weights(model_path / f\"{name_prefix}_episode_{episode}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(episode: int | None = None) -> int:\n",
    "    \"\"\"Load the model weights from the specified path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    episode : int | None\n",
    "        The episode to load the model weights from. If None, then the latest model weights will be loaded.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The episode number that the model weights were loaded from.\n",
    "    \"\"\"\n",
    "    if episode is None:\n",
    "        glob_path = model_path / \"*.h5\"\n",
    "        model_files = glob(str(glob_path))\n",
    "        episode = max([int(Path(model_file).stem.split(\"_\")[-1]) for model_file in model_files])\n",
    "    print(f\"Loading models from episode {episode}...\")\n",
    "    model.load_weights(model_path / f\"main_episode_{episode}.h5\")\n",
    "    target_model.load_weights(model_path / f\"target_episode_{episode}.h5\")\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a step counter and replay memory\n",
    "step_counter = 0\n",
    "replay_memory = deque(maxlen=10000)\n",
    "start_episode = load_models() + 1\n",
    "\n",
    "for episode in range(start_episode, num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = rgb2gray(state)\n",
    "    done = False\n",
    "    start_time = perf_counter()\n",
    "    while not done:\n",
    "        # Exploration-exploitation trade-off\n",
    "        exploration_threshold = random.uniform(0, 1)\n",
    "        # If exploration_threshold > exploration_rate, then exploitation\n",
    "        if exploration_threshold > exploration_rate:\n",
    "            q_values = model.predict(np.expand_dims(state, axis=0), verbose=0) # add batch dimension\n",
    "            action = np.argmax(q_values[0])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # Increment the step counter\n",
    "        step_counter += 1\n",
    "        \n",
    "        # Take action and observe the next state and reward\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = rgb2gray(next_state)\n",
    "\n",
    "        # Add the experience to replay memory\n",
    "        replay_memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Sample a minibatch from the replay buffer\n",
    "        if len(replay_memory) > batch_size:\n",
    "            minibatch = random.sample(replay_memory, batch_size)\n",
    "            # Train the model on the minibatch\n",
    "            train(model, target_model, minibatch, discount_rate)\n",
    "\n",
    "        if step_counter % update_target_network_steps == 0:\n",
    "            # Update the the target network with new weights\n",
    "            target_model.set_weights(model.get_weights())\n",
    "\n",
    "        # Decay the exploration rate\n",
    "        exploration_rate =  max(min_exploration_rate, exploration_rate * exploration_decay_rate)\n",
    "    # End of episode\n",
    "    print(f'Episode: {episode}, Exploration Rate: {exploration_rate:.2f}, Time: {perf_counter() - start_time:.2f}s')\n",
    "    if episode % 100 == 0:\n",
    "        # Save the model weights every 50 episodes\n",
    "        save_model(model, \"main\", episode)\n",
    "        save_model(target_model, \"target\", episode)\n",
    "    \n",
    "# Save the final model weights\n",
    "save_model(model, \"main\", episode)\n",
    "save_model(target_model, \"target\", episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the agent playing the game\n",
    "# env = gym.make(\"ALE/Tetris-v5\", render_mode='human')\n",
    "# state, _ = env.reset()\n",
    "# state = rgb2gray(state)\n",
    "# done = False\n",
    "# while not done:\n",
    "#     q_values = target_model.predict(np.expand_dims(state, axis=0), verbose=0) # add batch dimension\n",
    "#     action = np.argmax(q_values[0])\n",
    "#     next_state, reward, done, _, _ = env.step(action)\n",
    "#     next_state = rgb2gray(next_state)\n",
    "#     env.render()\n",
    "#     state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
